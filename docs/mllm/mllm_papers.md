# MLLM论文精选（持续更新）

## 最新动态
- 2024.08 [Building and better understanding vision-language models: insights and future directions](https://www.arxiv.org/pdf/2408.12637)
- 2024.08 [LongVILA: Scaling Long-Context Visual Language Models for Long Videos](https://arxiv.org/abs/2408.10188)
- 2024.08 [UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling](https://arxiv.org/pdf/2408.04810)
- 2024.08 [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://www.arxiv.org/abs/2408.08872) 
- 2024.08 [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) LLaVA-NeXT系列的集大成。
- 2024.08 [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800) 超强的小钢炮MLLM。
- 2024.08 [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714) 
- 2024.06 [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860) 提出以视觉为中心的benchmark CV-Bench。

## 经典论文
- 2021.02 [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020) CLIP
- 2022.04 [Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf) DeepMind出品，MLLM先驱。
- 2023.01 [BLIP-2](https://arxiv.org/abs/2301.12597) 提出Q-Former。
- 2023.03 [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) CLIP的变种替代品，Sigmoid损失。
- 2023.04 [MiniGPT-4](https://arxiv.org/abs/2304.10592) 热度很高。
- 2023.04 [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) LLaVA系列的第一篇文章。
- 2023.05 [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) 
- 2023.05 [Segment Anything](https://arxiv.org/abs/2304.02643) SAM
- 2023.12 [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805) 
- 2024.04 [MM1- Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) 苹果出品。
- 2024.05 [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247) Meta出品，短小精悍。
- 2024.05 [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/pdf/2403.05525)
- 2024.06 [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860) 提出以视觉为中心的benchmark CV-Bench，实验探究各个方面对VLM表现的影响，训练Cambrian-1模型。
