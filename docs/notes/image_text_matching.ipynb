{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2b847-be80-486f-9e1b-aa1a1bbee0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py#L175 。\n",
    "\n",
    "text_input_ids_world = concat_all_gather(text_tokens.input_ids)\n",
    "text_attention_mask_world = concat_all_gather(text_tokens.attention_mask)\n",
    "image_embeds_world = all_gather_with_grad(image_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ff69f-9081-4e21-9452-5d91f3ae2373",
   "metadata": {},
   "source": [
    "收集所有GPU的数据"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55adb44f-d2fa-4cda-9394-ee51416f860d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f6663-6af2-4b39-b6ad-e59cb673ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if \"image_id\" in samples.keys():\n",
    "        mask = torch.eq(image_ids, image_ids_all.t())\n",
    "        sim_t2i.masked_fill_(mask, -10000)\n",
    "        sim_i2t.masked_fill_(mask, -10000)\n",
    "    else:\n",
    "        sim_t2i[:, rank * bs : rank * bs + bs].fill_diagonal_(-10000)\n",
    "        sim_i2t[:, rank * bs : rank * bs + bs].fill_diagonal_(-10000)\n",
    "\n",
    "    weights_t2i = F.softmax(sim_t2i, dim=1)\n",
    "    weights_i2t = F.softmax(sim_i2t, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1fb69-b2ac-4be4-a545-b01d85922b01",
   "metadata": {},
   "source": [
    "计算文本到图像和图像到文本的相似度矩阵，并对相同ID的对进行掩码处理，以防止模型在负采样时选择相同的对。\n",
    "\n",
    "随后对相似度矩阵应用softmax，以获得每个文本对应不同图像的权重分布`weights_t2i`，以及每个图像对应不同文本的权重分布 `weights_i2t`。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "670f0477-578f-433f-806f-815d2a96e751",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11bacae-4965-4623-b757-ae10d7e4918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a negative image for each text\n",
    "image_embeds_neg = []\n",
    "for b in range(bs):\n",
    "    neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n",
    "    image_embeds_neg.append(image_embeds_world[neg_idx])\n",
    "image_embeds_neg = torch.stack(image_embeds_neg, dim=0)\n",
    "\n",
    "# select a negative text for each image\n",
    "text_ids_neg = []\n",
    "text_atts_neg = []\n",
    "for b in range(bs):\n",
    "    neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n",
    "    text_ids_neg.append(text_input_ids_world[neg_idx])\n",
    "    text_atts_neg.append(text_attention_mask_world[neg_idx])\n",
    "\n",
    "text_ids_neg = torch.stack(text_ids_neg, dim=0)\n",
    "text_atts_neg = torch.stack(text_atts_neg, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae11dc2-2f70-479c-8966-577e77d82c07",
   "metadata": {},
   "source": [
    "根据权重分布 `weights_t2i` 和 `weights_i2t` 为每个正样本选择一个负样本。具体做法是通过 `torch.multinomial` 从权重分布中采样负样本的索引，然后将这些负样本的嵌入（图像和文本）收集起来。\n",
    "\n",
    "通过 torch.multinomial 函数从这个分布中采样，可以有效地选择那些与当前文本（或图像）具有高相似度但实际并不匹配的负样本。这样就实现了hard negative mining，即选择那些难以区分的负样本来训练模型。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da15fe7c-26a9-41e6-8169-3fcd655bdd4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385100f-4f2b-4e30-a89d-8bf9d0ec1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids_all = torch.cat(\n",
    "    [text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0\n",
    ")  # pos, pos, neg\n",
    "text_atts_all = torch.cat(\n",
    "    [text_tokens.attention_mask, text_tokens.attention_mask, text_atts_neg],\n",
    "    dim=0,\n",
    ")\n",
    "\n",
    "query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1)\n",
    "query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to(\n",
    "    image.device\n",
    ")\n",
    "attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1)\n",
    "\n",
    "image_embeds_all = torch.cat(\n",
    "    [image_embeds, image_embeds_neg, image_embeds], dim=0\n",
    ")  # pos, neg, pos\n",
    "image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to(\n",
    "    image.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabac777-f508-4cb5-903c-d19be9c6e871",
   "metadata": {},
   "source": [
    " 将正样本和负样本的文本和图像拼接在一起，形成模型的输入。`text_ids_all` 和 `text_atts_all` 分别包含了正样本文本、正样本文本和负样本文本的 `input_ids` 和 `attention_mask`。\n",
    "\n",
    "`query_tokens_itm` 是用于ITM任务的查询标记，它们与文本和图像的嵌入一起输入模型。`attention_mask_all` 是拼接后的注意力掩码。\n",
    "\n",
    "`image_embeds_all` 是包含正样本图像、负样本图像和正样本图像的嵌入。`image_atts_all` 是对应的注意力掩码。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2211ef0a-091e-4954-9147-06f08e04cb00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69c6e9-76ac-4823-beb4-3e2fc3eefa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_itm = self.Qformer.bert(\n",
    "    text_ids_all,\n",
    "    query_embeds=query_tokens_itm,\n",
    "    attention_mask=attention_mask_all,\n",
    "    encoder_hidden_states=image_embeds_all,\n",
    "    encoder_attention_mask=image_atts_all,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb43c1-e4a1-4d9e-9fcc-9128a1d98931",
   "metadata": {},
   "source": [
    "前向传播。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e332930d-411e-4c39-8584-be7818d10133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baac244-5252-4df8-9919-c403d5050258",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :]\n",
    "vl_output = self.itm_head(vl_embeddings)\n",
    "logits = vl_output.mean(dim=1)\n",
    "\n",
    "itm_labels = torch.cat(\n",
    "    [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)],\n",
    "    dim=0,\n",
    ").to(image.device)\n",
    "loss_itm = F.cross_entropy(logits, itm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd040f-9c4f-4251-8960-1defa87db71f",
   "metadata": {},
   "source": [
    "提取出最后一层隐藏状态的查询嵌入，并通过线性分类器 `itm_head` 计算每个查询嵌入的logits。\n",
    "\n",
    "对所有查询嵌入的logits求平均，作为输出的匹配分数。\n",
    "\n",
    "制作标签 `itm_labels`，正样本为1，负样本为0。计算交叉熵损失 `loss_itm`。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
