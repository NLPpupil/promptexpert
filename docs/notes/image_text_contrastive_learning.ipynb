{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d91395-c8d2-4527-93e3-2215dabeb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py#L90\n",
    "image = samples[\"image\"]\n",
    "text = samples[\"text_input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726f1c9-6bce-4004-9984-7df434a3a149",
   "metadata": {},
   "source": [
    "image是图像的原始数据，shape是[batch_size, num_channels, height, width]。\n",
    "text是原始文本。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db9f7015-901c-4222-aa95-2ec1ab04497f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcb59a-cf13-4e65-9386-05afebfee0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = self.ln_vision(self.visual_encoder(image))\n",
    "image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edeae1-b5eb-4643-9149-5ace08245f42",
   "metadata": {},
   "source": [
    "visual_encoder就是Figure 2中的Image Encoder，比如EVA。image_embeds的shape是[batch_size, num_patches, embed_dim]\n",
    "image_atts将图片的attention全部设置为1，在之后的三个损失函数的计算中，图片特征都是可以全部互相注意的。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e91fdef3-709b-4ffc-80f2-6994f7d49cdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bb915-93ce-4cfb-90e7-e616880fc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bb6e7-33b8-432f-9695-de5f18072de9",
   "metadata": {},
   "source": [
    "query_tokens是示意图中的learned queries，扩张成image_embeds一样的batch size。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "773d8fb2-4b7d-4943-9eb6-a3f73232fa12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e7ac0-7d28-4a68-b118-5562912bed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output = self.Qformer.bert(\n",
    "    query_embeds=query_tokens,\n",
    "    encoder_hidden_states=image_embeds,\n",
    "    encoder_attention_mask=image_atts,\n",
    "    use_cache=True,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8134df3-b697-4506-b8eb-9ec34d8056d9",
   "metadata": {},
   "source": [
    "这部分的代码细节就开始多起来了。这里的bert是Q-Former的核心参数，其实Q-Former就是基于bert实现的。下文提到的bert，都是指self.Qformer.bert。\n",
    "\n",
    "bert的block层包含self attention层和cross attention层。它既可以接收query_embeds，也可以接收input_ids。如果只有query_embeds，就不需要过embedding层了。\n",
    "\n",
    "这里的encoder_hidden_states和encoder_attention_mask，和原始Transformer中的encoder不是一个东西。在Q-Former中，看到encoder_hidden_states和encoder_attention_mask，就说明有cross attention发生。\n",
    "\n",
    "详细的代码解释，见附录。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d66e7721-f353-4e2f-8193-d1ed897a09f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab815174-bc99-43a4-b1ca-196594bae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats = F.normalize(\n",
    "    self.vision_proj(query_output.last_hidden_state), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7212c-bba4-4519-8c51-119cd7f6669c",
   "metadata": {},
   "source": [
    "query_output.last_hidden_state是图片和queries计算完cross attention的隐状态。\n",
    "vision_proj是维度映射。\n",
    "image_feats是图片的最终特征。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "313d24bc-8033-4b6e-87a5-b2d9f2be397f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d10480-7a39-4c3c-a816-f64bc90400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = self.tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=self.max_txt_len,\n",
    "    return_tensors=\"pt\",\n",
    ").to(image.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b35523-3f88-45fb-934c-e41e3879e11b",
   "metadata": {},
   "source": [
    "tokenizer将原始文字转化为inpud_ids."
   ]
  },
  {
   "cell_type": "raw",
   "id": "65bdcf0c-f113-4990-9cd2-8aaa434506e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ff5b8-ebd3-4ea1-a4cf-e7648077bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_output = self.Qformer.bert(\n",
    "    text_tokens.input_ids,\n",
    "    attention_mask=text_tokens.attention_mask,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a8998-03b2-462b-82a7-81130948d0bc",
   "metadata": {},
   "source": [
    "bert是多功能的，在这里又可以编码文本。这就是原文所说的share the same self-attention layers。\n",
    "attention_mask和input_ids同形状，pad部分设为0，其他是1。也就是说，文本之前也是全部可以互相注意。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec6e730f-05b7-4424-a873-9e78807ac378",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf19429-f5c9-4932-9c61-4e256027cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feat = F.normalize(\n",
    "    self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba3aca-32e0-4b49-9fd9-2d25425e4d02",
   "metadata": {},
   "source": [
    "同理，text_feat是最终的文本特征。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "160cb8bd-cf31-4b1f-8680-f75f2dc8c6c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9a855-9014-4b1d-90d4-bd0f3153eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats_all = concat_all_gather(image_feats)  # [batch_size*num_gpu, num_query_tokens, embed_dim]\n",
    "text_feat_all = concat_all_gather(text_feat)  # [batch_size*num_gpu, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ca2a6-212c-4f35-a7f7-00c5c83ef696",
   "metadata": {},
   "source": [
    "将多个gpu的数据合并。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a3889bd-5854-4d6a-b3f4-7fcb6faf5ab2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623aea73-4dae-4054-8dc8-131af150dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_q2t = torch.matmul(\n",
    "    image_feats.unsqueeze(1), text_feat_all.unsqueeze(-1)\n",
    ").squeeze()\n",
    "# [batch_size, batch_size*num_gpu, num_query_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac130d1d-0319-44ef-846a-b35d973d691e",
   "metadata": {},
   "source": [
    "image_feats 的原始形状是 [batch_size, num_query_tokens, embed_dim]。unsqueeze(1) 在第1维度上增加一个维度，结果形状变为 [batch_size, 1, num_query_tokens, embed_dim]。\n",
    "\n",
    "text_feat_all 的原始形状是 [batch_size*num_gpu, embed_dim]。unsqueeze(-1) 在最后一个维度上增加一个维度，结果形状变为 [batch_size*num_gpu, embed_dim, 1]。\n",
    "\n",
    "torch.matmul将两个张量相乘。假设 A 的形状为 [a, b, c, d]，B 的形状为 [e, f, g]，如果 d 与 e 匹配，结果张量的形状为 [a, b, c, g]。\n",
    "在此上下文中：\n",
    "image_feats.unsqueeze(1) 的形状是 [batch_size, 1, num_query_tokens, embed_dim]\n",
    "text_feat_all.unsqueeze(-1) 的形状是 [batch_size*num_gpu, embed_dim, 1]\n",
    "通过矩阵乘法 torch.matmul，embed_dim 是匹配的维度，结果形状为 [batch_size, batch_size*num_gpu, num_query_tokens, 1]\n",
    "\n",
    "最终结果 sim_q2t 表示每个图像查询 token 与所有文本特征的相似度矩阵，其中：\n",
    "- 第一维度 batch_size 对应当前批次中的图像。\n",
    "- 第二维度 batch_size*num_gpu 对应所有收集到的文本特征（跨多个 GPU）。\n",
    "- 第三维度 num_query_tokens 对应每个图像的查询 token。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4935d00-bb60-43d9-95e0-df1bf8552c17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d8b7f-d302-42af-afb9-569df403af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image-text similarity: aggregate across all query tokens\n",
    "sim_i2t, _ = sim_q2t.max(-1)\n",
    "sim_i2t = sim_i2t / self.temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3755736-4ff2-475b-84ba-1edc05ad01fb",
   "metadata": {},
   "source": [
    "计算最大值，得到[batch_size, batch_size*num_gpu]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cb8239c-3bee-4edf-8198-23fc87a71cc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ae165-0176-468c-bbd2-e3ed099e0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-query similarity: [batch_size, batch_size*num_gpu, num_query_tokens]\n",
    "sim_t2q = torch.matmul(\n",
    "    text_feat.unsqueeze(1).unsqueeze(1), image_feats_all.permute(0, 2, 1)\n",
    ").squeeze()\n",
    "\n",
    "# text-image similarity: aggregate across all query tokens\n",
    "sim_t2i, _ = sim_t2q.max(-1)\n",
    "sim_t2i = sim_t2i / self.temp  # [batch_size, batch_size*num_gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06226198-5732-4318-80c1-4cfb7ce93515",
   "metadata": {},
   "source": [
    "同理。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9165570c-2f0f-4bbe-bc11-7a01f2886ed8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4f10b-3b3f-46e7-8899-6eaf4ecaff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = dist.get_rank()\n",
    "bs = image.size(0)\n",
    "targets = torch.linspace(rank * bs, rank * bs + bs - 1, bs, dtype=int).to(image.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26042ab-b3ed-4988-921f-b1d58ecf9c35",
   "metadata": {},
   "source": [
    "torch.linspace(start, end, steps, dtype) 生成一个从 start 到 end 的等差数列，共有 steps 个元素。\n",
    "这里生成的是目标标签，具体解释如下：\n",
    "- start 是 rank * bs，即当前进程（或 GPU）的排名乘以批次大小。这是当前批次的起始索引。\n",
    "- end 是 rank * bs + bs - 1，即当前批次的结束索引。\n",
    "- steps 是 bs，即生成 bs 个目标标签。\n",
    "\n",
    "假设有4个 GPU，每个 GPU 的批次大小为 32。\n",
    "- 对于 rank=0 的 GPU，生成的目标标签为 [0, 1, 2, ..., 31]。\n",
    "- 对于 rank=1 的 GPU，生成的目标标签为 [32, 33, 34, ..., 63]。\n",
    "- 对于 rank=2 的 GPU，生成的目标标签为 [64, 65, 66, ..., 95]。\n",
    "- 对于 rank=3 的 GPU，生成的目标标签为 [96, 97, 98, ..., 127]。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfbb1f7e-8f23-443f-aafb-1bd05f5e9e52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582b217-6a79-4263-9104-cfb1a891e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"image_id\" in samples.keys(): #coco retrieval finetuning\n",
    "    image_ids = samples[\"image_id\"].view(-1,1)\n",
    "    image_ids_all = concat_all_gather(image_ids)\n",
    "    pos_idx = torch.eq(image_ids, image_ids_all.t()).float()       \n",
    "    sim_targets = pos_idx / pos_idx.sum(1,keepdim=True)   \n",
    "    sim_targets = 0.9 * sim_targets + 0.1 * torch.ones_like(sim_targets) / sim_targets.size(1)\n",
    "\n",
    "    loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_targets,dim=1).mean()\n",
    "    loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_targets,dim=1).mean()     \n",
    "    loss_itc = (loss_t2i+loss_i2t)/2  \n",
    "else:                     \n",
    "    loss_itc = (\n",
    "        F.cross_entropy(sim_i2t, targets, label_smoothing=0.1)\n",
    "        + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1)\n",
    "    ) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42a9a1-7dbf-490a-a076-f8c9f123564a",
   "metadata": {},
   "source": [
    "解释else部分。\n",
    "sim_i2t 是图像到文本的相似度矩阵，形状为 [batch_size, batch_size*num_gpu]，表示当前批次中的每个图像与所有文本特征的相似度。\n",
    "sim_t2i 是文本到图像的相似度矩阵，形状为 [batch_size, batch_size*num_gpu]，表示当前批次中的每个文本与所有图像特征的相似度。\n",
    "交叉熵计算的是负对数，targets告诉损失函数每个图像（或文本）对应的正确文本（或图像）的索引。最小化交叉熵，就相当于最大化图片和其对应文本的相似度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
