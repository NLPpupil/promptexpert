{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2a079-2749-448f-88ce-d05744a9af23",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_extended_attention_mask(\n",
    "        self,\n",
    "        attention_mask: Tensor,\n",
    "        input_shape: Tuple[int],\n",
    "        device: device,\n",
    "        is_decoder: bool,\n",
    "        has_query: bool = False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529e2b9",
   "metadata": {},
   "source": [
    "get_extended_attention_mask用于生成扩展的attention mask。该函数特别适用于处理self-attention和causal mask情况，尤其是在处理解码器decoder时。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b25e072-e580-40bb-bf2c-94e9f8365a88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9e32a-d15b-4bbe-a8cd-c14463838466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "# ourselves in which case we just need to make it broadcastable to all heads.\n",
    "if attention_mask.dim() == 3:\n",
    "    extended_attention_mask = attention_mask[:, None, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae49bd-0b1f-444e-8b00-64f535aa877e",
   "metadata": {},
   "source": [
    "Transformer模型中的多头自注意力机制需要一个形状为 [batch_size, num_heads, from_seq_length, to_seq_length] 的掩码，其中 num_heads 是注意力头的数量。通过在第二个维度插入一个新的维度（None），我们可以确保 attention_mask 变得可广播到所有注意力头上。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ecdb8a-4572-4468-ad0f-adab0a256ede",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939167a-afef-4afc-9dc1-c5808ff7d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif attention_mask.dim() == 2:\n",
    "    # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "    # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "    # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddf2a4-0086-4c4f-a2f7-14d96cc0ff41",
   "metadata": {},
   "source": [
    "Padding Mask：用于标记输入序列中的填充位置。填充位置通常是为了使序列长度一致，但这些位置不应该在计算注意力得分时被考虑。\n",
    "Causal Mask：用于确保解码器只能关注当前和之前的标记，以防止信息泄露。这对于自回归生成模型尤为重要，因为模型在生成下一个标记时不应该访问未来的标记。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "371eb5d9-cdc4-403f-a467-038c44a8bf2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf7d45-7efb-4bbd-bb03-fbb961affd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_decoder:\n",
    "    batch_size, seq_length = input_shape\n",
    "\n",
    "    seq_ids = torch.arange(seq_length, device=device)\n",
    "    causal_mask = (\n",
    "        seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n",
    "        <= seq_ids[None, :, None]\n",
    "    )\n",
    "\n",
    "    # add a prefix ones mask to the causal mask\n",
    "    # causal and attention masks must have same type with pytorch version < 1.3\n",
    "    causal_mask = causal_mask.to(attention_mask.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763725d2-820e-47f2-aa6c-57f627fd9ac4",
   "metadata": {},
   "source": [
    "这段代码的目的是生成一个causal mask，以确保每个位置只能关注到它之前的位置。\n",
    "seq_ids[None, None, :]通过添加两个维度变成形状为(1, 1, seq_length)。\n",
    "seq_ids[None, None, :].repeat(batch_size, seq_length, 1)将其重复成形状为(batch_size, seq_length, seq_length)。\n",
    "seq_ids[None, :, None]通过添加两个维度变成形状为(1, seq_length, 1)。\n",
    "比较操作<=会生成一个布尔张量causal_mask，形状为(batch_size, seq_length, seq_length)。对于每个位置(i, j)，如果i <= j，则设为True，否则设为False。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7032bbd6-a518-4089-8a32-bd8802b17272",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1721911-f6c6-4ea4-8108-d2153c3bd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "    if has_query:  # UniLM style attention mask\n",
    "        causal_mask = torch.cat(\n",
    "            [\n",
    "                torch.zeros(\n",
    "                    (batch_size, prefix_seq_len, seq_length),\n",
    "                    device=device,\n",
    "                    dtype=causal_mask.dtype,\n",
    "                ),\n",
    "                causal_mask,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091cc7a-aeaf-44cc-9998-9861bd8c1fc3",
   "metadata": {},
   "source": [
    "attention_mask所有位置的mask，causal_mask只是causal部分的mask，causal_mask的长度可能小于attention_mask的长度。\n",
    "如果这种情况是因为存在query(即论文中的learned query)，需要把query这部分的mask设为0。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00abbe76-df20-4cbc-97b5-89f1daa771fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16525a0-deeb-4dcb-b5b5-a3018a1a5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.cat(\n",
    "        [\n",
    "            torch.ones(\n",
    "                (batch_size, causal_mask.shape[1], prefix_seq_len),\n",
    "                device=device,\n",
    "                dtype=causal_mask.dtype,\n",
    "            ),\n",
    "            causal_mask,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492c606-fc56-4413-9cce-22a0f368a758",
   "metadata": {},
   "source": [
    "否则，prefix的mask设为1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d569517-3bb7-4305-8ba3-1e5670a33303",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b277a4e-1ea2-44e2-b530-7bb40d88ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_attention_mask = (\n",
    "    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0bcca-9cb6-4287-b7dd-58da71d9ffc0",
   "metadata": {},
   "source": [
    "将两个mask广播后相乘，就是前面说的apply a causal mask in addition to the padding mask。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
