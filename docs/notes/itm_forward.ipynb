{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ce6682-5940-4a04-83f0-7d765e022126",
   "metadata": {},
   "source": [
    "# https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py#L175\n",
    "\n",
    "主要变量含义：\n",
    "- image_embeds_neg：\n",
    "    - 含义：这是为每个文本选择的负样本图像的embedding。\n",
    "    - 形状：[bs, D]，其中 bs 是批次大小（batch size），D是图像嵌入的维度。每个元素都是一个负样本图像的嵌入向量。\n",
    "- text_ids_neg：\n",
    "    - 含义：这是为每个图像选择的负样本文本的输入ID。\n",
    "    - 形状：[bs, L]，其中 L 是文本序列的长度。每个元素是一个负样本文本的输入ID序列。\n",
    "- text_atts_neg：\n",
    "    - 含义：这是为每个图像选择的负样本文本的mask。所有非pad未知都是1.\n",
    "    - 形状：[bs, L]，其中 L 是文本序列的长度。每个元素是一个负样本文本的注意力掩码序列。\n",
    "- text_ids_all：\n",
    "    - 含义：这是组合后的所有文本输入ID，包括正样本的文本ID（两次）和负样本的文本ID。\n",
    "    - 形状：[3 * bs, L]，其中 bs 是批次大小（batch size），L 是文本序列的长度。\n",
    "        - 第一个 bs：正样本的文本ID，来源于 text_tokens.input_ids。\n",
    "        - 第二个 bs：再次包含正样本的文本ID，仍然来源于 text_tokens.input_ids。\n",
    "        - 第三个 bs：负样本的文本ID，来源于 text_ids_neg。\n",
    "- text_atts_all：\n",
    "    - 含义：这是组合后的所有文本注意力掩码，包括正样本的注意力掩码（两次）和负样本的注意力掩码。\n",
    "    - 形状：[3 * bs, L]，其中 bs 是批次大小（batch size），L 是文本序列的长度。\n",
    "        - 第一个 bs：正样本的注意力掩码，来源于 text_tokens.attention_mask。\n",
    "        - 第二个 bs：再次包含正样本的注意力掩码，仍然来源于 text_tokens.attention_mask。\n",
    "        - 第三个 bs：负样本的注意力掩码，来源于 text_atts_neg。\n",
    "- query_tokens_itm:\n",
    "    - 含义：扩展后的query tokens\n",
    "    - 形状：和text_ids_all同形状，即[3 * bs, L]\n",
    "- query_atts_itm:\n",
    "    - 含义：都是1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae943645-9935-4908-bbcf-202223d483a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672a1d9-7ccf-40c4-b648-21d0df4af9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620e911-b6f9-485e-a82d-118322836b02",
   "metadata": {},
   "source": [
    "queries和文本的mask拼接为一个整体。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "074627b6-8041-461b-a03c-dcfa9b2bc9a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6442d-13ce-4479-bc7e-441faa1f80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_all = torch.cat(\n",
    "            [image_embeds, image_embeds_neg, image_embeds], dim=0\n",
    ")  # pos, neg, pos\n",
    "image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to(\n",
    "    image.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8927da-1c1d-4a85-a523-0907d9ec63bf",
   "metadata": {},
   "source": [
    "image_embeds_all也是三个batch的拼接。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1493a6ee-6042-418a-b651-2d0354ad7f43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcac0b-4fa7-4c5b-bb43-a8df0cc3511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_itm = self.Qformer.bert(\n",
    "    text_ids_all,\n",
    "    query_embeds=query_tokens_itm,\n",
    "    attention_mask=attention_mask_all,\n",
    "    encoder_hidden_states=image_embeds_all,\n",
    "    encoder_attention_mask=image_atts_all,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "# BertModel forward\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    query_embeds=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_values=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "    is_decoder=False,\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05e4a7-7b4b-4b50-818e-a2c44b7b1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=text_ids_all\n",
    "attention_mask=attention_mask_all\n",
    "query_embeds=query_tokens_itm\n",
    "encoder_hidden_states=image_embeds_all\n",
    "encoder_attention_mask=image_atts_all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ce0ca21-3d42-41c5-9a79-1f4b54011b29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b299a-7238-490e-96a1-a72075b0365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel forward\n",
    "query_length = query_embeds.shape[1] if query_embeds is not None else 0\n",
    "\n",
    "embedding_output = self.embeddings(\n",
    "    input_ids=input_ids,\n",
    "    position_ids=position_ids,\n",
    "    query_embeds=query_embeds,\n",
    "    past_key_values_length=past_key_values_length,\n",
    ")\n",
    "\n",
    "input_shape = embedding_output.size()[:-1]\n",
    "\n",
    "if is_decoder:\n",
    "    # ...\n",
    "else:\n",
    "    extended_attention_mask = self.get_extended_attention_mask(\n",
    "        attention_mask, input_shape, device, is_decoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23539ed0-064c-4a8d-95df-5f5182bf3cc9",
   "metadata": {},
   "source": [
    "query_length=query数量\n",
    "\n",
    "embedding_output = torch.cat((query_embeds, input_embeddings), dim=1)\n",
    "\n",
    "get_extended_attention_mask和图文对比学习的类似。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83938781-7324-4e5d-a4cf-e4db4cbe281e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c705b92-32dd-4d11-acb5-8cf3f567298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel forward\n",
    "if encoder_hidden_states is not None:\n",
    "    if type(encoder_hidden_states) == list:\n",
    "        # ...\n",
    "    else:\n",
    "        (\n",
    "            encoder_batch_size,\n",
    "            encoder_sequence_length,\n",
    "            _,\n",
    "        ) = encoder_hidden_states.size()\n",
    "    encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "\n",
    "    if type(encoder_attention_mask) == list:\n",
    "        # ...\n",
    "    elif encoder_attention_mask is None:\n",
    "        # ...\n",
    "    else:\n",
    "        encoder_extended_attention_mask = self.invert_attention_mask(\n",
    "            encoder_attention_mask\n",
    "        )\n",
    "else:\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34826924-abf8-4d1a-a4f4-569f4a42f9f5",
   "metadata": {},
   "source": [
    "假设 encoder_attention_mask 是一个形如 [0, 0, 1, 1, 1] 的序列，在调用 invert_attention_mask 之后，这个序列可能会被变换成 [1, 1, 0, 0, 0]，或者在一些实现中，可能被变换成一个很大的负数（如 -10000）以确保在 softmax 计算时，这些位置的权重几乎为零。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99452631-01c6-482f-8ab7-cadefadc8894",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd77a0-ae63-47c4-a48c-3a78bbed2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel forward\n",
    "encoder_outputs = self.encoder(\n",
    "    embedding_output,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    head_mask=head_mask,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    encoder_attention_mask=encoder_extended_attention_mask,\n",
    "    past_key_values=past_key_values,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict,\n",
    "    query_length=query_length,\n",
    ")\n",
    "\n",
    "# BertEncoder forward\n",
    "def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_values=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    return_dict=True,\n",
    "    query_length=0,\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efb71b-d047-4ad1-8258-2ab1a9ec3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertEncoder forward\n",
    "hidden_states=embedding_output\n",
    "attention_mask=extended_attention_mask\n",
    "encoder_hidden_states=image_embeds_all\n",
    "encoder_attention_mask=image_atts_all\n",
    "query_length=query数量"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a29432-785b-45cc-82a8-6750c0feb532",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62643171-646a-488c-9dc2-5d6fac073943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertEncoder forward\n",
    "for i in range(self.config.num_hidden_layers):\n",
    "    layer_module = self.layer[i]\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "    past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "    if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "        # ...\n",
    "    else:\n",
    "        layer_outputs = layer_module(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            layer_head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "            query_length,\n",
    "        )\n",
    "\n",
    "# BertLayer的forward方法\n",
    "def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    past_key_value=None,\n",
    "    output_attentions=False,\n",
    "    query_length=0,\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9599b-39ff-4647-97d1-28ef53ff606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer的forward方法\n",
    "hidden_states=embedding_output\n",
    "attention_mask=extended_attention_mask\n",
    "encoder_hidden_states=image_embeds_all\n",
    "encoder_attention_mask=image_atts_all\n",
    "query_length=query数量"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10bc3117-f2ce-4ba8-ad64-788ff1cfaec9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c988cc-e190-4318-be70-a9f9a91fdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertLayer的forward方法\n",
    "self_attention_outputs = self.attention(\n",
    "    hidden_states,\n",
    "    attention_mask,\n",
    "    head_mask,\n",
    "    output_attentions=output_attentions,\n",
    "    past_key_value=self_attn_past_key_value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f5577-a7b8-4e60-a887-9239ce85ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
